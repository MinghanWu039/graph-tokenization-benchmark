# Benchmarking for Graph Tokenization Methods

## What We Want to Know

How to use graph tokenization method to generate sequences from graphs that are suitable as inputs for transformers? How does this approach compare with SoTA graph transformer method like GraphGPS?

*In short, benchmarking graph learning methods for several graph tasks*,

## Papers \& Code

### Key Results used in Benchmarking

- [Recipe for a General, Powerful, Scalable Graph Transformer](https://arxiv.org/abs/2205.12454) i.e. GraphGPS

- [Flatten Graphs as Sequences: Transformers are Scalable Graph Generators](https://arxiv.org/abs/2502.02216) i.e. Autograph

- [graph-token](https://github.com/alip67/graph-token)

- MPNN

### Other foundamental papers

- [GAT](https://arxiv.org/abs/1710.10903)
- [Graph transformer](https://arxiv.org/abs/2012.09699)
- [Attending to Graph Transformers](https://arxiv.org/abs/2302.04181)

## Evaluation Pipeline

Tasks: 
1) Cycle check on a synthetic dataset 
2) Shortest-path distance estimation on a synthetic dataset
3) Graph classification on ZINC 12k subset

Approaches: 
1) GraphGPS 
2) MPNN 
3) tokenization via autograph -> transformer
4) tokenization via graph-token -> transformer

Training scripts: 
1) GraphGPS codebase for GraphGPS 
2) Custom pytorch-geometric for MPNN 
3) Custom transformer training for tokenization approaches

Data source: Synthetic graphs generated by [graph-token](https://github.com/alip67/graph-token).