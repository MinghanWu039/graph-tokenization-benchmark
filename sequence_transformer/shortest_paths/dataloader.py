"""PyTorch dataset/dataloader utilities for shortest-path sequence modeling.

The source data directory must contain `train.txt` and `test.txt` files where
each line follows the format generated by `graph_token_shortest_paths.py`:

    <graph_index> <graph_sequence_tokens ...> <start end> <distance>

The sequence tokens run from `<bos>` up to the `<start end>` marker (inclusive);
the final field on each line is the distance label. This module loads the data,
splits the training file into train/validation subsets (default 80/20), and
creates padded batches ready for transformer models.
"""

from __future__ import annotations

import os
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Tuple

import torch
from torch.utils.data import Dataset, DataLoader, Subset


# ---------------------------------------------------------------------------
# Vocabulary
# ---------------------------------------------------------------------------


@dataclass
class Vocabulary:
    """Simple token -> id mapping with PAD/UNK reserved tokens."""

    pad_token: str = "<pad>"
    unk_token: str = "<unk>"

    def __post_init__(self) -> None:
        self.stoi: Dict[str, int] = {self.pad_token: 0, self.unk_token: 1}
        self.itos: Dict[int, str] = {0: self.pad_token, 1: self.unk_token}

    def build(self, tokens: Sequence[str]) -> None:
        for token in tokens:
            if token not in self.stoi:
                idx = len(self.stoi)
                self.stoi[token] = idx
                self.itos[idx] = token

    def encode(self, tokens: Sequence[str]) -> List[int]:
        return [self.stoi.get(tok, self.stoi[self.unk_token]) for tok in tokens]

    def __len__(self) -> int:
        return len(self.stoi)

    @property
    def pad_id(self) -> int:
        return self.stoi[self.pad_token]


# ---------------------------------------------------------------------------
# Dataset
# ---------------------------------------------------------------------------


class BaseShortestPathDataset(Dataset):
    """Base dataset for shortest-path sequences."""

    def __init__(
        self,
        file_path: str,
        vocab: Optional[Vocabulary] = None,
        max_length: Optional[int] = None,
        expect_start_end: bool = False,
    ) -> None:
        if not os.path.isfile(file_path):
            raise FileNotFoundError(file_path)

        self.file_path = file_path
        self.max_length = max_length
        self.samples: List[Tuple[str, List[str], int]] = []
        self.expect_start_end = expect_start_end

        tokens_seen: List[str] = []
        with open(file_path, "r") as handle:
            for lineno, line in enumerate(handle, start=1):
                line = line.strip()
                if not line:
                    continue
                graph_index, seq_tokens, label = self._parse_line(parts=line.split(), lineno=lineno)

                if max_length is not None and len(seq_tokens) > max_length:
                    seq_tokens = seq_tokens[:max_length]

                tokens_seen.extend(seq_tokens)
                self.samples.append((graph_index, seq_tokens, label))

        if not self.samples:
            raise ValueError(f"No samples loaded from {file_path}")

        if vocab is None:
            vocab = Vocabulary()
            vocab.build(tokens_seen)
        else:
            # Ensure the vocabulary contains any unseen tokens.
            vocab.build(tokens_seen)
        self.vocab = vocab

    def _parse_line(self, parts: List[str], lineno: int) -> Tuple[str, List[str], int]:
        if self.expect_start_end:
            if len(parts) < 4:
                raise ValueError(f"Malformed line {lineno} in {self.file_path}: '{' '.join(parts)}'")
            graph_index = parts[0]
            label_str = parts[-1]
            seq_tokens = parts[1:-1]
        else:
            if len(parts) < 3:
                raise ValueError(f"Malformed line {lineno} in {self.file_path}: '{' '.join(parts)}'")
            graph_index = parts[0]
            seq_tokens = parts[1:-2]
            start_end = parts[-2]
            label_str = parts[-1]
            seq_tokens = seq_tokens + [start_end]
        try:
            label = int(label_str)
        except ValueError as exc:
            raise ValueError(f"Invalid label on line {lineno} in {self.file_path}: {exc}") from exc
        return graph_index, seq_tokens, label

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> Dict[str, object]:
        graph_index, tokens, label = self.samples[idx]
        token_ids = self.vocab.encode(tokens)
        return {
            "graph_index": graph_index,
            "tokens": tokens,
            "input_ids": token_ids,
            "label": label,
            "length": len(token_ids),
        }


class ShortestPathDataset(BaseShortestPathDataset):
    """Dataset for graph-token sequences (generated by graph_token_shortest_paths)."""

    def __init__(self, file_path: str, vocab: Optional[Vocabulary] = None, max_length: Optional[int] = None) -> None:
        super().__init__(file_path, vocab=vocab, max_length=max_length, expect_start_end=True)


class AutographShortestPathDataset(BaseShortestPathDataset):
    """Dataset for Autograph sequences where `<start end>` appears before the label."""

    def __init__(self, file_path: str, vocab: Optional[Vocabulary] = None, max_length: Optional[int] = None) -> None:
        super().__init__(file_path, vocab=vocab, max_length=max_length, expect_start_end=False)


def make_collate_fn(
    vocab: Vocabulary,
    max_length: Optional[int] = None,
) -> callable:
    """Create a collate function for DataLoader."""

    def _collate(batch: List[Dict[str, object]]) -> Dict[str, torch.Tensor]:
        batch_lengths = [min(sample["length"], max_length) if max_length else sample["length"] for sample in batch]
        target_len = max(batch_lengths)
        if max_length is not None:
            target_len = min(target_len, max_length)

        input_ids = []
        attention_mask = []
        labels = []
        graph_ids = []
        graph_id_strings = []

        for sample in batch:
            seq = sample["input_ids"]
            if len(seq) > target_len:
                seq = seq[:target_len]
            pad_len = target_len - len(seq)
            input_ids.append(seq + [vocab.pad_id] * pad_len)
            attention_mask.append([1] * len(seq) + [0] * pad_len)
            labels.append(sample["label"])
            graph_id_strings.append(sample["graph_index"])
            try:
                graph_ids.append(int(sample["graph_index"]))
            except (ValueError, TypeError):
                graph_ids.append(-1)

        batch = {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "labels": torch.tensor(labels, dtype=torch.long),
            "graph_index": torch.tensor(graph_ids, dtype=torch.long),
            "lengths": torch.tensor(batch_lengths, dtype=torch.long),
        }
        batch["graph_index_str"] = graph_id_strings
        return batch

    return _collate


# ---------------------------------------------------------------------------
# Dataset splitting utilities
# ---------------------------------------------------------------------------


def split_dataset(dataset: Dataset, val_fraction: float = 0.2, seed: int = 42) -> Tuple[Subset, Subset]:
    """Split a dataset into train/val subsets."""
    if not 0.0 < val_fraction < 1.0:
        raise ValueError("val_fraction must be between 0 and 1.")

    n = len(dataset)
    indices = list(range(n))
    random.Random(seed).shuffle(indices)
    split_idx = int(n * (1.0 - val_fraction))
    train_idx = indices[:split_idx]
    val_idx = indices[split_idx:]
    return Subset(dataset, train_idx), Subset(dataset, val_idx)


# ---------------------------------------------------------------------------
# Public loader factory
# ---------------------------------------------------------------------------


def create_dataloaders(
    data_root: str,
    batch_size: int = 32,
    max_length: Optional[int] = None,
    val_fraction: float = 0.2,
    num_workers: int = 0,
    pin_memory: bool = False,
    type="graph-token",
    seed: int = 42,
) -> Tuple[DataLoader, DataLoader, DataLoader, Vocabulary]:
    """Create train/val/test DataLoaders for the shortest path dataset."""

    root = Path(data_root)
    train_file = root / "train.txt"
    test_file = root / "test.txt"
    if not train_file.exists() or not test_file.exists():
        raise FileNotFoundError(f"Expected train.txt and test.txt under {root}")

    if type == "graph-token":
        train_dataset = ShortestPathDataset(str(train_file), vocab=None, max_length=max_length)
        train_subset, val_subset = split_dataset(train_dataset, val_fraction=val_fraction, seed=seed)

        test_dataset = ShortestPathDataset(str(test_file), vocab=train_dataset.vocab, max_length=max_length)
    elif type == "autograph":
        train_dataset = AutographShortestPathDataset(str(train_file), vocab=None, max_length=max_length)
        train_subset, val_subset = split_dataset(train_dataset, val_fraction=val_fraction, seed=seed)

        test_dataset = AutographShortestPathDataset(str(test_file), vocab=train_dataset.vocab, max_length=max_length)
    else:
        raise ValueError(f"Unknown dataset type: {type}")

    collate = make_collate_fn(train_dataset.vocab, max_length=max_length)

    train_loader = DataLoader(
        train_subset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        collate_fn=collate,
    )
    val_loader = DataLoader(
        val_subset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        collate_fn=collate,
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        collate_fn=collate,
    )

    return train_loader, val_loader, test_loader, train_dataset.vocab
